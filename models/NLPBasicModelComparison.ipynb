{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag, word_tokenize\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing Paper Titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlabelled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to OAuth2 credentials JSON file\n",
    "creds_path = r'C:\\Users\\cathe\\practical-data-science-tutorial\\src\\data\\credentials.json'\n",
    "\n",
    "# Define the name of Google Sheet\n",
    "google_sheet_name = 'Draft-dataset'\n",
    "\n",
    "# Define the name of the specific sheet within the Google Sheet\n",
    "specific_sheet_names = ['Health and medical sciences', 'Social sciences', 'Business, economics and management']\n",
    "\n",
    "# Initialize the Google Sheets client\n",
    "scope = ['https://spreadsheets.google.com/feeds',\n",
    "         'https://www.googleapis.com/auth/drive']\n",
    "creds = ServiceAccountCredentials.from_json_keyfile_name(creds_path, scope)\n",
    "client = gspread.authorize(creds)\n",
    "\n",
    "papers_unlabelled = []\n",
    "\n",
    "for specific_sheet_name in specific_sheet_names:\n",
    "    # Open the Google Sheet\n",
    "    sheet = client.open(google_sheet_name).worksheet(specific_sheet_name)\n",
    "\n",
    "    # Get list of article names from the sheet\n",
    "    papers = sheet.col_values(3)[1:]\n",
    "\n",
    "    # Add all article names to a single list\n",
    "    papers_unlabelled.extend(papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The psychological impact of quarantine and how to reduce it: rapid review of the evidence',\n",
       " 'Global, regional, and national incidence, prevalence, and years lived with disability for 354 diseases and injuries for 195 countries and territories, 1990â€“2017: a systematic analysis for the Global Burden of Disease Study 2017',\n",
       " 'A novel coronavirus outbreak of global health concern']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers_unlabelled[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labelled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the name of Google Sheet\n",
    "google_sheet_name = 'Draft-dataset'\n",
    "\n",
    "# Define the name of the specific sheet within the Google Sheet\n",
    "specific_sheet_name = 'Copy of finaldataset'\n",
    "\n",
    "# Initialize the Google Sheets client\n",
    "scope = ['https://spreadsheets.google.com/feeds',\n",
    "         'https://www.googleapis.com/auth/drive']\n",
    "creds = ServiceAccountCredentials.from_json_keyfile_name(creds_path, scope)\n",
    "client = gspread.authorize(creds)\n",
    "\n",
    "# Open the Google Sheet\n",
    "sheet = client.open(google_sheet_name).worksheet(specific_sheet_name)\n",
    "\n",
    "# Get list of article names & labels from the sheet\n",
    "papers_labelled = sheet.col_values(3)[1:]\n",
    "labels = sheet.col_values(15)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [int(i) for i in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['COVID-19: the gendered impacts of the outbreak',\n",
       "  'COVID-19: towards controlling of a pandemic',\n",
       "  'Prevention and treatment of low back pain: evidence, challenges, and promising directions'],\n",
       " [3, 1, 3])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers_labelled[:3], labels[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\cathe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\cathe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\cathe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\cathe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Download NLTK data for stopwords and lemmatisation\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Define stopwords to remove\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['covid19', 'sarscov2'])\n",
    "\n",
    "# Initialize lemmatiser\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Map POS tags to WordNet tags for lemmatisation\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def preprocess_title(title):\n",
    "    # Remove non-alphanumeric characters\n",
    "    title_alpha = re.sub(r'[^a-zA-Z0-9\\s]', '', title)\n",
    "\n",
    "    # Convert to lowercase\n",
    "    title_lower = title_alpha.lower()\n",
    "\n",
    "    # Break down into individual words\n",
    "    words = word_tokenize(title_lower)\n",
    "\n",
    "    # Remove stopwords\n",
    "    # words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    # Part-of-Speech (POS) tagging\n",
    "    tagged_words = pos_tag(words)\n",
    "\n",
    "    # Remove words that are nouns (NN, NNS, NNP, NNPS)\n",
    "    non_noun_words = [word for word, pos in tagged_words if not pos.startswith('N')]\n",
    "\n",
    "    # Lemmatisation of words & removal of words that aren't adjectives/verbs/adverbs\n",
    "    lemmatised_words = [lemmatizer.lemmatize(word) for word in non_noun_words]\n",
    "\n",
    "    return lemmatised_words\n",
    "\n",
    "# Apply preprocessing to all titles\n",
    "papers_labelled_processed = [preprocess_title(title) for title in papers_labelled]\n",
    "papers_unlabelled_processed = [preprocess_title(title) for title in papers_unlabelled]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['covid19', 'the', 'gendered', 'of', 'the'],\n",
       " ['controlling', 'of', 'a', 'pandemic'],\n",
       " ['and', 'of', 'low', 'back', 'and', 'promising']]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers_labelled_processed[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the',\n",
       "  'psychological',\n",
       "  'of',\n",
       "  'and',\n",
       "  'how',\n",
       "  'to',\n",
       "  'reduce',\n",
       "  'it',\n",
       "  'rapid',\n",
       "  'of',\n",
       "  'the'],\n",
       " ['global',\n",
       "  'regional',\n",
       "  'and',\n",
       "  'national',\n",
       "  'and',\n",
       "  'lived',\n",
       "  'with',\n",
       "  'for',\n",
       "  '354',\n",
       "  'and',\n",
       "  'for',\n",
       "  '195',\n",
       "  'and',\n",
       "  '19902017',\n",
       "  'a',\n",
       "  'systematic',\n",
       "  'for',\n",
       "  'the',\n",
       "  'global',\n",
       "  'of',\n",
       "  '2017'],\n",
       " ['a', 'novel', 'of', 'global']]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers_unlabelled_processed[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CountVectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit the vectorizer on preprocessed titles and transform them into a BoW representation\n",
    "labelled_bow = vectorizer.fit_transform([\" \".join(title) for title in papers_labelled_processed])\n",
    "\n",
    "# Store the vocabulary used for creating BoW representations\n",
    "vocabulary = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<100x288 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 623 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelled_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CountVectorizer object\n",
    "vectorizer = CountVectorizer(vocabulary=vocabulary)\n",
    "\n",
    "# Fit the vectorizer on preprocessed titles and transform them into a BoW representation\n",
    "unlabelled_bow = vectorizer.fit_transform([\" \".join(title) for title in papers_unlabelled_processed])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1242x288 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 5290 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabelled_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'max_depth': 30, 'n_estimators': 100}\n",
      "Cross-validation scores: [0.45 0.4  0.35 0.5  0.65]\n",
      "Validation weighted F1: 0.55\n",
      "Validation accuracy: 0.55\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into a training set and a validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(labelled_bow, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define a range of hyperparameters for grid search\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 1, 2, 3, 5, 8, 10, 20, 30]\n",
    "}\n",
    "\n",
    "# Initialize the Random Forest model\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Perform grid search with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(rf_model, param_grid, cv=5, scoring='f1_weighted')\n",
    "\n",
    "# Fit the model to find the best hyperparameters\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best hyperparameters: {best_params}\")\n",
    "best_rf_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the validation set using the best model\n",
    "y_pred = best_rf_model.predict(X_val)\n",
    "\n",
    "# Calculate metrics\n",
    "f1 = f1_score(y_val, y_pred, average='weighted')\n",
    "print(f\"Validation weighted F1: {f1}\")\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f\"Validation accuracy: {accuracy}\")\n",
    "\n",
    "# Predict labels for unlabelled data using the best model\n",
    "predicted_scores = best_rf_model.predict(unlabelled_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
